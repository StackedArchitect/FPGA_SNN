#!/usr/bin/env python3
"""
Weight Quantization for FPGA Implementation
============================================
Converts floating-point weights to fixed-point integers for hardware.

This script handles:
- Scaling float weights to integer range
- Handling signed values (inhibitory connections)
- Ensuring hardware compatibility (bit width constraints)

Author: Senior FPGA Engineer
Date: January 28, 2026
"""

import numpy as np
import json


class WeightQuantizer:
    """Quantize floating-point weights to fixed-point integers"""
    
    def __init__(self, bit_width=8, scale_factor=100):
        """
        Initialize quantizer
        
        Args:
            bit_width: Number of bits for fixed-point representation
            scale_factor: Multiplier for float to int conversion
        """
        self.bit_width = bit_width
        self.scale_factor = scale_factor
        
        # Calculate valid range for signed integers
        self.max_value = (2 ** (bit_width - 1)) - 1
        self.min_value = -(2 ** (bit_width - 1))
        
        print("=" * 70)
        print("Weight Quantizer Initialized")
        print("=" * 70)
        print(f"Bit Width: {bit_width} bits")
        print(f"Scale Factor: {scale_factor}")
        print(f"Value Range: [{self.min_value}, {self.max_value}]")
        print("=" * 70 + "\n")
    
    def quantize(self, weight_float):
        """
        Convert floating-point weight to integer
        
        Args:
            weight_float: Floating-point weight value
            
        Returns:
            Quantized integer weight
        """
        # Scale float to integer
        weight_int = int(weight_float * self.scale_factor)
        
        # Clamp to valid range
        if weight_int > self.max_value:
            print(f"WARNING: Weight {weight_float} ({weight_int}) exceeds max. Clamping to {self.max_value}")
            weight_int = self.max_value
        elif weight_int < self.min_value:
            print(f"WARNING: Weight {weight_float} ({weight_int}) below min. Clamping to {self.min_value}")
            weight_int = self.min_value
        
        return weight_int
    
    def quantize_network(self, weights_dict):
        """
        Quantize all weights in a network
        
        Args:
            weights_dict: Dictionary of weight names to float values
            
        Returns:
            Dictionary of quantized weights
        """
        quantized = {}
        
        print("Quantizing Network Weights:")
        print("-" * 70)
        print(f"{'Weight Name':<20} | {'Float Value':>12} | {'Quantized':>10}")
        print("-" * 70)
        
        for name, value in weights_dict.items():
            q_value = self.quantize(value)
            quantized[name] = q_value
            print(f"{name:<20} | {value:>12.4f} | {q_value:>10d}")
        
        print("-" * 70 + "\n")
        
        return quantized
    
    def generate_verilog_params(self, quantized_weights, filename="weights.vh"):
        """
        Generate Verilog parameter file
        
        Args:
            quantized_weights: Dictionary of quantized weights
            filename: Output file name
        """
        with open(filename, 'w') as f:
            f.write("// Auto-generated Verilog Weight Parameters\n")
            f.write("// Generated by quantize_weights.py\n")
            f.write(f"// Date: January 28, 2026\n\n")
            
            for name, value in quantized_weights.items():
                param_name = name.upper().replace('-', '_')
                f.write(f"parameter signed [{self.bit_width-1}:0] {param_name} = {value};\n")
        
        print(f"Generated Verilog parameter file: {filename}\n")
    
    def save_json(self, quantized_weights, filename="weights.json"):
        """Save quantized weights to JSON file"""
        with open(filename, 'w') as f:
            json.dump(quantized_weights, f, indent=2)
        
        print(f"Saved quantized weights to: {filename}\n")


def main():
    """Example: Quantize XOR network weights"""
    
    print("\n" + "=" * 70)
    print(" WEIGHT QUANTIZATION FOR XOR SNN")
    print("=" * 70 + "\n")
    
    # Example floating-point weights (from training or tuning)
    # In practice, these might come from a trained network
    float_weights = {
        "w_i0_h0": 0.85,      # 85 after scaling
        "w_i1_h0": 0.90,      # 90 after scaling
        "w_i0_h1": 0.80,      # 80 after scaling
        "w_i1_h1": 0.85,      # 85 after scaling
        "w_h0_o": 0.75,       # 75 after scaling
        "w_h1_o": 0.70,       # 70 after scaling
        "w_h0_h1": -0.95,     # -95 after scaling (inhibitory)
        "w_h1_h0": -0.90,     # -90 after scaling (inhibitory)
    }
    
    # For the XOR case, we can also use the already-integer weights from model_xor.py
    integer_weights_direct = {
        "w_i0_h0": 20,
        "w_i1_h0": 20,
        "w_i0_h1": 20,
        "w_i1_h1": 20,
        "w_h0_o": 15,
        "w_h1_o": 15,
        "w_h0_h1": -25,  # Inhibitory
        "w_h1_h0": -25,  # Inhibitory
    }
    
    # Initialize quantizer
    quantizer = WeightQuantizer(bit_width=8, scale_factor=1)  # Scale=1 since already integers
    
    # Quantize weights
    print("Using pre-tuned integer weights from model_xor.py:\n")
    quantized = quantizer.quantize_network(integer_weights_direct)
    
    # Generate Verilog parameter file
    quantizer.generate_verilog_params(quantized, "weights.vh")
    
    # Save to JSON
    quantizer.save_json(quantized, "weights.json")
    
    # Display summary
    print("=" * 70)
    print(" QUANTIZATION COMPLETE")
    print("=" * 70)
    print("Files generated:")
    print("  - weights.vh   (Verilog parameters)")
    print("  - weights.json (JSON format)")
    print("\nThese weights can now be used in your FPGA design!")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()
